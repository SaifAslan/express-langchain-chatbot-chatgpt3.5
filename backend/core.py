# from dotenv import load_dotenv
# import os

# load_dotenv()

# from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# from typing import Any, Dict, List
# from  langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
# from langchain_pinecone import PineconeVectorStore




# def run_llm(query: str, chat_history: List[Dict[str, Any]] = []):
#     embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
#     docsearch = PineconeVectorStore(embedding=embeddings, index_name=os.environ["INDEX_NAME"])

#     chat = ChatOpenAI(
#         verbose=True,
#         temperature=0,
#     )

#     qa = ConversationalRetrievalChain.from_llm(
#         llm=chat, retriever=docsearch.as_retriever(), return_source_documents=True
#     )
#     return qa.invoke(input={"question": query, "chat_history": chat_history})

# if __name__ == "__main__":
#     print(run_llm(query="What is main differences between express versions?")['answer'])

# Import necessary libraries
from dotenv import load_dotenv
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain_pinecone import PineconeVectorStore
from typing import Any, Dict, List

# Load environment variables
load_dotenv()

def init_embeddings() -> OpenAIEmbeddings:
    """
    This function initializes the OpenAI embeddings model.
    
    Returns:
    OpenAIEmbeddings: The initialized embeddings model.
    """
    return OpenAIEmbeddings(model="text-embedding-3-small")

def init_vector_store(embeddings: OpenAIEmbeddings, index_name: str) -> PineconeVectorStore:
    """
    This function initializes the Pinecone vector store.
    
    Args:
    embeddings (OpenAIEmbeddings): The embeddings model used to convert documents to vectors.
    index_name (str): The name of the index in the Pinecone vector store.
    
    Returns:
    PineconeVectorStore: The initialized vector store.
    """
    return PineconeVectorStore(embedding=embeddings, index_name=index_name)

def init_chat() -> ChatOpenAI:
    """
    This function initializes the ChatOpenAI model.
    
    Returns:
    ChatOpenAI: The initialized chat model.
    """
    return ChatOpenAI(
        verbose=True,
        temperature=0,
    )

def init_conversational_retrieval_chain(chat: ChatOpenAI, docsearch: PineconeVectorStore) -> ConversationalRetrievalChain:
    """
    This function initializes the Conversational Retrieval Chain.
    
    Args:
    chat (ChatOpenAI): The chat model used to generate responses.
    docsearch (PineconeVectorStore): The vector store used to retrieve relevant documents.
    
    Returns:
    ConversationalRetrievalChain: The initialized conversational retrieval chain.
    
    Note:
    The Conversational Retrieval Chain uses a prompt to rephrase the user's follow-up question based on the chat history.
    The prompt is: 
    "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.
    
    Chat History:
    {chat_history}
    Follow Up Input: {question}
    Standalone question:"
    
    This prompt is used to condense the follow-up question into a standalone question.
    
    Another prompt is used to answer the standalone question:
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    {context}
    
    Question: {question}
    Helpful Answer:"
    """
    return ConversationalRetrievalChain.from_llm(
        llm=chat, retriever=docsearch.as_retriever(), return_source_documents=True
    )

def run_llm(query: str, chat_history: List[Dict[str, Any]] = []) -> Dict[str, Any]:
    """
    This function runs the LLM to generate a response to the user's query based on the chat history.
    
    Args:
    query (str): The user's query.
    chat_history (List[Dict[str, Any]], optional): The chat history. Defaults to [].
    
    Returns:
    Dict[str, Any]: The response generated by the LLM.
    """
    embeddings = init_embeddings()
    docsearch = init_vector_store(embeddings, os.environ["INDEX_NAME"])
    chat = init_chat()
    qa = init_conversational_retrieval_chain(chat, docsearch)
    return qa.invoke(input={"question": query, "chat_history": chat_history})

if __name__ == "__main__":
    # Test the run_llm function
    print(run_llm(query="What is main differences between express versions?")['answer'])